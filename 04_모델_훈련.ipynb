{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0. 모델의 훈련__\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수를 계산하여 모델의 각 $\\theta$, w 등으로 불리는 파라미터들을 조정해 나가는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. 선형 회귀__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.1 행렬 계산__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __1.1.1 정규 방정식__\n",
    "> $\\hat\\theta = (X^{T}X)^{-1}X^{T}y$  \n",
    "\n",
    "손실함수 MSE를 사용할 때, 이 공식을 사용하여 비용함수를 최소화하는 파라미터들을 얻을 수 있습니다.  \n",
    "하지만, 역행렬이 존재해야하며, 데이터의 수(행)가 특성 수(열)보다 많아야합니다.  \n",
    "또한 특성수가 많은 머신러닝에서 (n+1)x(n+1)의 행렬 $X^{Y}X$의 역행렬을 계산하는 것 또한 힘든일이 될 수 있습니다.  \n",
    "> mxn 행렬에서, 정규방정식의 시간 복잡도 c는 $O(n^{2.4}) <= c <= O(n^{3}), O(m)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___정규 방정식 증명___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} = w_{0}+w_{1}x_{i}+\\epsilon_{i}$ 형태로 이루어진 행렬 $Y = Xw$에서,  \n",
    "MSE $ = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})^2$이며,  \n",
    "이를 최소화 하기 위해 가중치(파라미터) $w_{1}, w_{2}$를 각각 편미분해주고 0으로하는 w를 찾는다.  \n",
    "\n",
    "${\\delta \\over \\delta w_{0}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})=0$   \n",
    "${\\delta\\over \\delta w_{1}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})x_{i}=0$  \n",
    "\n",
    "위를 정리하여,  \n",
    "$w_{0}n + w_{1}\\sum_{i}^{n}{x_{i}}= \\sum_{i}^{n}{y_{i}}$  \n",
    "$w_{0}\\sum_{i}^{n}{x_{i}} + w_{1}\\sum_{i}^{n}{x_{i}}^2 = \\sum_{i}^{n}{y_{i}}x_{i}$  \n",
    "\n",
    "두 좌변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}n&\\sum_{i}^{n}{x_{i}}\\\\\\sum_{i}^{n}{x_{i}}&\\sum_{i}^{n}{x_{i}}^2\\\\ \\end{bmatrix} \\begin{bmatrix} w_{0}\\\\w_{1}\\\\ \\end{bmatrix} = X^{T}Xw$  \n",
    "\n",
    "두 우변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}\\sum_{i}^{n}{y_{i}}\\\\ \\sum_{i}^{n}{y_{i}}x_{i}\\\\ \\end{bmatrix} = X^{T}Y$  \n",
    "\n",
    "즉, 이 둘을 정리하면,  \n",
    "$X^{T}Xw = X^{T}Y$  \n",
    "$ \\mathbf{\\therefore w = (X^{T}X)^{-1}X^{T}Y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.1.2 유사역행렬__\n",
    "> $\\hat\\theta = X^{+}y$  \n",
    "$X^{+} = V\\Sigma^{-1}U^{T}$   \n",
    "$* V, \\Sigma, U$는 특이값분해의 값  \n",
    "\n",
    "이 방식을 사용하면 가역 여부, 행열의 수 비교 등의 제약 없이 모든 행렬에 가능하며 연산속도도 빠릅니다.\n",
    "> mxn 행렬에서, 유사역행렬의 시간 복잡도는 $O(n^2), O(m)$  \n",
    "\n",
    "행렬로 표기할 경우, mxn행렬에서  \n",
    "m>n인 경우 정규방정식과 동일하게 $X^{+} = (X^{T}X)^{-1}X^{T}$이 되며,  \n",
    "m<n인 경우, $X^{+} = X^{T}(XX^{T})^{-1}$이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.2 경사하강법__\n",
    "> Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산이 아닌 반복적인 학습을 통해 매번 파라미터를 조정하여 점진적으로 손실함수를 줄여나가는 방법입니다.  \n",
    "각 특성에 대해 손실함수를 미분하여 기울기 값을 계산한 뒤, 그 값이 0에 수렴하도록 조정하면 최소값을 찾을 수 있습니다.  \n",
    "> $\\theta_{(j)i+1} = \\theta_{(j)i}-\\eta \\Delta Loss(\\theta_{(j)})$  \n",
    "$\\Delta Loss(\\theta_{(j)}) = (\\theta^{T}\\mathbf{x}-y){x_{(j)}}$\n",
    "\n",
    "-{1 \\over m}\\sum_{i}^{m}{[y_{i}log(\\hat{p_i})+(1-y_{i})log(\\hat{1-p_i})]}$ 수정 !!!\n",
    "\n",
    "* $\\eta$, __학습률__ : 한번의 학습으로 손실함수가 조정되는 정도입니다.  \n",
    "  너무 작으면 지역 최솟값에 고정되거나 많은 학습이 필요하고, 너무 크면 미분값이 발산할 수 있습니다.  \n",
    "\n",
    "*  ___주의사항___  \n",
    "  데이터를 스케일링 하는 작업은 수렴을 빠르게 하기 위한 것입니다.  \n",
    "  그리고 손실함수의 미분이 0인 지점이 여러개 있을 수 있다는 것을 염두해야합니다.   \n",
    "  이렇게 미분이 0이지만 최솟값이 아닌 지점을 <b>지역 최솟값</b>이라고 하며, 진짜 최솟값을 <b>전역 최솟값</b>이라고 합니다.  \n",
    "> 손실함수 MSE의 경우 지역최솟값이 없습니다.\n",
    "  \n",
    "학습의 반복 횟수와 지역최솟값 여부를 알 수 없기 때문에 미분값이 허용오차($\\epsilon$) 이하가 될때까지 학습을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.1 배치 경사하강법__\n",
    "> BGD. Batch Gradient Descent\n",
    "\n",
    "학습마다 모든 데이터의 미분값을 계산하는 방식입니다.  \n",
    "안정적으로 최솟값을 찾을 수 있지만, 지역최솟값을 경우 이를 벗어나기 힘듭니다.  \n",
    "또한 데이터양이 많아질수록 오래걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.2 확률적 경사하강법__\n",
    "> SGD. Stochastic Gradient Descent\n",
    "\n",
    "한번의 학습에 데이트 세트 중 하나의 샘플을 랜덤하게 골라 그것만의 미분값을 계산하는 방식입니다.  \n",
    "> 랜덤하게 골라야 전체 데이터 세트에 대해 편향을 최소화하고 대표할 수 있습니다.  \n",
    "\n",
    "데이터 양에 상관없이 빠르기 때문에 편리합니다.  \n",
    "배치 방식에 비해 불안정하여 최적의 값은 찾기 힘들지만, 그로 인해 지역최솟값을 피할 확률도 높습니다.  \n",
    "따라서 학습률을 점진적으로 감소시켜 지역최솟값을 피하면서 최종적으로는 어떤 최솟값에 근접하도록 할 수 있습니다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.3 미니배치 경사하강법__\n",
    "데이터 세트 중에 임의의 샘플세트로 미분값을 계산하여 조정하는 방식입니다.  \n",
    "> SGD에 비해, 행렬연산이 강한 GPU를 사용할 시 얻는 성능향상이 큽니다.  \n",
    "\n",
    "SGD보다 안정적으로 조정하여 지역최솟값에 도달하기도 쉽지만, 어떤 최솟값에 더욱 근접하게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.3. 다항회귀__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "각 샘플마다 값의 제곱, 교차항의 곱을 추가하여 선형회귀를 계산하는 방식입니다.  \n",
    "이렇게하면 직선으로 표현 불가능한 <b>비선형 데이터세트</b>를 모델링할 수 있습니다.  \n",
    "> 교차항의 값이 추가되기 때문에 각 특성간의 관계 또한 찾을 수 있습니다.  \n",
    "\n",
    "> `sklearn.preprocessing.PolynomialFeatures'로 데이터를 변환한 뒤 선형회귀를 합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플마다 값의 제곱, 교차항의 곱을 추가하여 선형회귀를 계산하는 방식입니다.  \n",
    "이렇게하면 직선으로 표현 불가능한 <b>비선형 데이터세트</b>를 모델링할 수 있습니다.  \n",
    "> 교차항의 값이 추가되기 때문에 각 특성간의 관계 또한 찾을 수 있습니다.  \n",
    "\n",
    "> `sklearn.preprocessing.PolynomialFeatures'로 데이터를 변환한 뒤 선형회귀를 합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2. 적합성과 규제__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2.1 과소적합__\n",
    "모델이 학습을 덜 하여, 훈련/검증 데이터 간의 성능 차이는 적지만, 성능 자체가 낮은 경우입니다.  \n",
    "학습샘플의 양은 중요하지 않으며, 더 복잡한 모델이나 특성을 추가하여 개선할 수 있습니다.\n",
    "> 각 성능을 반복횟수(훈련세트 크기)로 그래프를 그려, 성능이 나쁘고 간격이 적으면 과소적합입니다.  \n",
    "\n",
    "### __2.2 과대적합__\n",
    "모델이 학습데이터만 과도하게 조정되어 훈련데이터의 성능은 좋지만 검증데이터의 성능이 나쁜 경우입니다.  \n",
    "고차 다항회귀 등과 같이 모델이 복잡한 경우에 이러한 경향을 띠며,  \n",
    "모델의 복잡도를 낮추거나, 학습샘플를 더 많이 추가하면 개선할 수 있습니다.\n",
    "> 각 성능을 반복횟수(훈련세트 크기)로 그래프를 그려, 간격이 넓으면 과대적합입니다.  \n",
    "\n",
    "### __2.3 규제__\n",
    "모델의 복잡도를 줄이는 방법들을 총칭합니다.  \n",
    "다항회귀의 경우 차수를 낮추고, 선형회귀의 경우 가중치를 제한합니다.  \n",
    "손실함수와 성능지표가 같아도 규제는 훈련하는 동안만 사용하며, 최종 평가는 규제 없이 평가합니다.  \n",
    "\n",
    "#### __2.3.1 L2-정규화, 릿지 회귀 (티호노프 규제)__\n",
    "규제항 $\\alpha \\sum_{i=1}^{m}{\\theta_{i}^2}$을 손실함수에 더하는 방식입니다.  \n",
    "> $\\alpha$: 규제량으로, 높을수록 회귀직선의 기울기가 0으로 수렴합니다.  \n",
    "  가중치 벡터 w에서, 규제항은 유클리디안 norm을 사용하여 ${1\\over2}(\\lVert w \\rVert _2)^2$입니다.  \n",
    "\n",
    "파라미터들은 가중치에 비례하여 0으로 증감합니다. 그에따라 학습률 또한 진행될수록 작아집니다.  \n",
    "따라서 일반적으로 잘 쓰이는 규제입니다. \n",
    "\n",
    "#### __2.3.2 L1-정규화, 라소 회귀__\n",
    "규제항 $\\alpha \\sum_{i=1}^{m}|{\\theta_{i}|}$을 손실함수에 더하는 방식입니다.  \n",
    "> 가중치 벡터 w에서, 규제항은 맨해튼 norm을 사용하여 $\\lVert w \\rVert _1$입니다.  \n",
    "\n",
    "모든 파라미터를 동일한 만큼 0으로 증감시키기 때문에, 중요하지 않은 특성들은 가중치가 0으로 됩니다.  \n",
    "또한, 스텝이 진행되어도 학습률이 변화하지 않아, 손실함수가 정확히 0이 되지는 않습니다.  \n",
    "\n",
    "몇몇의 파라미터만 중요하다고 생각하면 사용하며,  \n",
    "특성이 샘플보다 많거나 소수의 특성이 너무 중요하면 문제가 생길 수 있습니다.\n",
    "\n",
    "맨해튼 노름은 0에서 미분이 불가능하기 때문에 하방미분의 계산값 <b>서브그레디언트 벡터</b>라는 것을 사용합니다.  \n",
    "> 하방미분: 미분 불가능한 지점에서, 미분가능한 양 극한의 미분값의 평균값\n",
    "\n",
    "#### __2.3.3 엘라스틱넷__\n",
    "릿지와 라소의 혼합형 모델로, 두 규제 모두를 더하는 방식입니다.  \n",
    "> $r\\alpha \\sum_{i=1}^{m}|{\\theta_{i}|} + {{1-r}\\over 2}\\alpha \\sum_{i=1}^{m}{\\theta_{i}^2}$\n",
    "\n",
    "라소를 사용하려고 했는데 문제가 있을 수 있기 때문에 라소 대신에 사용합니다.\n",
    "\n",
    "#### __2.3.4 조기종료__\n",
    "반복적인 학습 알고리즘에서, 검증에러가 가장 적어질 때 까지만 학습하고 그만두는 방법입니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3. 분류에 사용하는 회귀__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3.1 로지스틱 회귀__\n",
    "한 샘플이 특정 클래스에 속할 확률을 추정하는 알고리즘으로 이진분류, 예측 모두 사용합니다.  \n",
    "일반 선형회귀와 동일하게 작동하지만, 결과값을 <b>시그모이드 함수</b>를 통해 확률값으로 변환합니다.  \n",
    "이 방법 역시 규제를 할 수 있습니다.  \n",
    "* __시그모이드 함수의 종류__  \n",
    "  __로지스틱 함수__: 베르누이 실행에서의 승산비에 로그를 취한 것의 역함수   \n",
    "  > $\\sigma(x) = {1\\over1+exp(-x)}$  \n",
    "  \n",
    "  __하이퍼볼릭 탄젠트__: 로지스틱 함수에서 상하로 2배 늘리고, 좌우로 2배 줄인 함수\n",
    "  > $tanh(x)=2\\sigma (2x) -1$\n",
    "\n",
    "#### __3.1.1 손실함수__\n",
    "한 샘플에 대한 손실은 -log(x)로, x는 y가 1일때 $\\hat p$, 0일때 $1-\\hat p$입니다.   \n",
    "따라서 (미니)배치의 손실함수는 아래와 같습니다.\n",
    "> $logloss(\\theta) = -{1 \\over m}\\sum_{i}^{m}{[y_{i}log(\\hat{p_i})+(1-y_{i})log(\\hat{1-p_i})]}$\n",
    "\n",
    "이 함수의 미분값은 컨벡스하기에 경사하강법을 통한 전역최솟값을 보장합니다.  \n",
    "> ${\\delta \\over \\delta \\theta_{(j)}}logloss(\\theta_{(j)}) = {1 \\over m} \\sum_{i}^{m}{(\\sigma (\\theta^{T}\\mathbf{x_{i}})-y_{i})x_{(j)i}}$  \n",
    "j = 특성 갯수, m = 샘플 갯수\n",
    "\n",
    "* __결정경계__: 특정 경계를 기준으로 양성이 몇 퍼센트인지를 나누는 1차식 직선입니다.  \n",
    "\n",
    "### __3.2 소프트맥스 회귀__\n",
    "<b>다항 로지스틱 회귀</b>라고도 불리우는 소프트맥스는 로지스틱 회귀를 여러번 사용하여 다중클래스를 분류합니다.  \n",
    "한 샘플에 대해, 각 클래스에 속할 확률을 계산하고 <b>소프트맥스 함수</b>를 적용하여 가장 높은 확률의 클래스로 분류합니다.  \n",
    "> $softmax(x)={e^{x} \\over \\sum{e^{x_i}}}$  \n",
    "\n",
    "소프트맥스는 각 클래스별 확률의 총합이 1이 되도록 변환하는 함수로,  \n",
    "자연상수를 사용하여 미분을 편리하게 하고, 큰 값을 더 크게, 작은 값을 더 작게하여 구분이 쉬워집니다.  \n",
    "n=2 일때 (클래스가 2개일 때), 시그모이드와 동일합니다.  \n",
    "\n",
    "#### __3.2.1 크로스엔트로피 손실함수__\n",
    "타겟 클래스에 대해 확률이 1일 때 최소가 되는 함수입니다.   \n",
    "> $crossEntropy(w) = -{1 \\over m}\\sum_{i}^{m}\\sum_{i}^{k}{y_{i}log(\\hat{p}_{i})}$  \n",
    "  k = 클래스 갯수\n",
    "\n",
    "> ${\\delta \\over \\delta w}crossEntropy(w) = {1 \\over m}\\sum_{i}^{m}(\\hat{p_{k}}-y_{k})\\mathbf x$\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704561b0d644309cd2ab0220a5b729490d45ccecc249cd05cdfebaf802484c8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
