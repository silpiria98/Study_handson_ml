{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. 모델의 훈련__\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'모델을 훈련 시킨다', '모델이 학습한다'라는 것은 모델 안의 파라미터들을 조정해나가는 것입니다.  \n",
    "모델을 평가하는 <b>손실(비용)함수</b> $\\theta$로 모델을 평가하여 이를 <b>최소화</b>하는 과정을 모델의 훈련이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.1 행렬 계산__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __1.1.1 정규 방정식__\n",
    "> $\\hat\\theta = (X^{T}X)^{-1}X^{T}y$  \n",
    "\n",
    "이 공식을 사용하여 비용함수를 최소화하는 파라미터들을 얻을 수 있습니다.  \n",
    "하지만, 역행렬이 존재해야하며, 데이터의 수(행)가 특성 수(열)보다 많아야합니다.  \n",
    "또한 특성수가 많은 머신러닝에서 (n+1)x(n+1)의 행렬 $X^{Y}X$의 역행렬을 계산하는 것 또한 힘든일이 될 수 있습니다.  \n",
    "> mxn 행렬에서, 정규방정식의 시간 복잡도 c는 $O(n^{2.4}) <= c <= O(n^{3}), O(m)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___정규 방정식 증명___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} = w_{0}+w_{1}x_{i}+\\epsilon_{i}$ 형태로 이루어진 행렬 $Y = Xw$에서,  \n",
    "MSE $ = \\theta = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})^2$이며,  \n",
    "이를 최소화 하기 위해 가중치(파라미터) $w_{1}, w_{2}$를 각각 편미분해주고 0으로하는 w를 찾는다.  \n",
    "\n",
    "${\\delta \\theta \\over \\delta w_{0}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})=0$   \n",
    "${\\delta \\theta \\over \\delta w_{1}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})x_{i}=0$  \n",
    "\n",
    "위를 정리하여,  \n",
    "$w_{0}n + w_{1}\\sum_{i}^{n}{x_{i}}= \\sum_{i}^{n}{y_{i}}$  \n",
    "$w_{0}\\sum_{i}^{n}{x_{i}} + w_{1}\\sum_{i}^{n}{x_{i}}^2 = \\sum_{i}^{n}{y_{i}}x_{i}$  \n",
    "\n",
    "두 좌변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}n&\\sum_{i}^{n}{x_{i}}\\\\\\sum_{i}^{n}{x_{i}}&\\sum_{i}^{n}{x_{i}}^2\\\\ \\end{bmatrix} \\begin{bmatrix} w_{0}\\\\w_{1}\\\\ \\end{bmatrix} = X^{T}Xw$  \n",
    "\n",
    "두 우변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}\\sum_{i}^{n}{y_{i}}\\\\ \\sum_{i}^{n}{y_{i}}x_{i}\\\\ \\end{bmatrix} = X^{T}Y$  \n",
    "\n",
    "즉, 이 둘을 정리하면,  \n",
    "$X^{T}Xw = X^{T}Y$  \n",
    "$ \\mathbf{\\therefore w = (X^{T}X)^{-1}X^{T}Y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.1.2 유사역행렬__\n",
    "> $\\hat\\theta = X^{+}y$  \n",
    "$X^{+} = V\\Sigma^{-1}U^{T}$   \n",
    "$* V, \\Sigma, U$는 특이값분해의 값  \n",
    "\n",
    "이 방식을 사용하면 가역 여부, 행열의 수 비교 등의 제약 없이 모든 행렬에 가능하며 연산속도도 빠릅니다.\n",
    "> mxn 행렬에서, 유사역행렬의 시간 복잡도는 $O(n^2), O(m)$  \n",
    "\n",
    "행렬로 표기할 경우, mxn행렬에서  \n",
    "m>n인 경우 정규방정식과 동일하게 $X^{+} = (X^{T}X)^{-1}X^{T}$이 되며,  \n",
    "m<n인 경우, $X^{+} = X^{T}(XX^{T})^{-1}$이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.2 경사하강법__\n",
    "계산이 아닌 반복적인 학습을 통해 매번 파라미터를 조정하여 점진적으로 손실함수를 줄여나가는 방법입니다.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704561b0d644309cd2ab0220a5b729490d45ccecc249cd05cdfebaf802484c8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
