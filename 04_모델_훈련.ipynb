{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0. 모델의 훈련__\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수 $\\theta$를 계산하여 모델의 각 파라미터들을 조정해 나가는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. 선형 회귀__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.1 행렬 계산__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __1.1.1 정규 방정식__\n",
    "> $\\hat\\theta = (X^{T}X)^{-1}X^{T}y$  \n",
    "\n",
    "이 공식을 사용하여 비용함수를 최소화하는 파라미터들을 얻을 수 있습니다.  \n",
    "하지만, 역행렬이 존재해야하며, 데이터의 수(행)가 특성 수(열)보다 많아야합니다.  \n",
    "또한 특성수가 많은 머신러닝에서 (n+1)x(n+1)의 행렬 $X^{Y}X$의 역행렬을 계산하는 것 또한 힘든일이 될 수 있습니다.  \n",
    "> mxn 행렬에서, 정규방정식의 시간 복잡도 c는 $O(n^{2.4}) <= c <= O(n^{3}), O(m)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___정규 방정식 증명___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} = w_{0}+w_{1}x_{i}+\\epsilon_{i}$ 형태로 이루어진 행렬 $Y = Xw$에서,  \n",
    "MSE $ = \\theta = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})^2$이며,  \n",
    "이를 최소화 하기 위해 가중치(파라미터) $w_{1}, w_{2}$를 각각 편미분해주고 0으로하는 w를 찾는다.  \n",
    "\n",
    "${\\delta \\theta \\over \\delta w_{0}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})=0$   \n",
    "${\\delta \\theta \\over \\delta w_{1}} = \\sum_{i}^{n} (w_{1}x_{i}+w_{0}-y_{i})x_{i}=0$  \n",
    "\n",
    "위를 정리하여,  \n",
    "$w_{0}n + w_{1}\\sum_{i}^{n}{x_{i}}= \\sum_{i}^{n}{y_{i}}$  \n",
    "$w_{0}\\sum_{i}^{n}{x_{i}} + w_{1}\\sum_{i}^{n}{x_{i}}^2 = \\sum_{i}^{n}{y_{i}}x_{i}$  \n",
    "\n",
    "두 좌변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}n&\\sum_{i}^{n}{x_{i}}\\\\\\sum_{i}^{n}{x_{i}}&\\sum_{i}^{n}{x_{i}}^2\\\\ \\end{bmatrix} \\begin{bmatrix} w_{0}\\\\w_{1}\\\\ \\end{bmatrix} = X^{T}Xw$  \n",
    "\n",
    "두 우변을 행렬로 나타내어,  \n",
    "$\\begin{bmatrix}\\sum_{i}^{n}{y_{i}}\\\\ \\sum_{i}^{n}{y_{i}}x_{i}\\\\ \\end{bmatrix} = X^{T}Y$  \n",
    "\n",
    "즉, 이 둘을 정리하면,  \n",
    "$X^{T}Xw = X^{T}Y$  \n",
    "$ \\mathbf{\\therefore w = (X^{T}X)^{-1}X^{T}Y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.1.2 유사역행렬__\n",
    "> $\\hat\\theta = X^{+}y$  \n",
    "$X^{+} = V\\Sigma^{-1}U^{T}$   \n",
    "$* V, \\Sigma, U$는 특이값분해의 값  \n",
    "\n",
    "이 방식을 사용하면 가역 여부, 행열의 수 비교 등의 제약 없이 모든 행렬에 가능하며 연산속도도 빠릅니다.\n",
    "> mxn 행렬에서, 유사역행렬의 시간 복잡도는 $O(n^2), O(m)$  \n",
    "\n",
    "행렬로 표기할 경우, mxn행렬에서  \n",
    "m>n인 경우 정규방정식과 동일하게 $X^{+} = (X^{T}X)^{-1}X^{T}$이 되며,  \n",
    "m<n인 경우, $X^{+} = X^{T}(XX^{T})^{-1}$이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.2 경사하강법__\n",
    "> Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산이 아닌 반복적인 학습을 통해 매번 파라미터를 조정하여 점진적으로 손실함수를 줄여나가는 방법입니다.  \n",
    "손실함수를 미분하여 기울기 값을 계산한 뒤, 그 값이 0에 수렴하도록 조정하면 최소값을 찾을 수 있습니다.  \n",
    "\n",
    "*  ___주의사항___\n",
    "  데이터를 스케일링 하는 작업은 수렴을 빠르게 하기 위한 것입니다.  \n",
    "  그리고 손실함수의 미분이 0인 지점이 여러개 있을 수 있다는 것을 염두해야합니다.   \n",
    "  이렇게 미분이 0이지만 최솟값이 아닌 지점을 <b>지역 최솟값</b>이라고 하며, 진짜 최솟값을 <b>전역 최솟값</b>이라고 합니다.  \n",
    "> 손실함수 MSE의 경우 지역최솟값이 없습니다.\n",
    "\n",
    "* __학습률__ : 한번의 학습으로 손실함수가 조정되는 정도입니다.  \n",
    "  너무 작으면 지역 최솟값에 고정되거나 많은 학습이 필요하고, 너무 크면 미분값이 발산할 수 있습니다.  \n",
    "  \n",
    "학습의 반복 횟수와 지역최솟값 여부를 알 수 없기 때문에 미분값이 허용오차($\\epsilon$) 이하가 될때까지 학습을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.1 배치 경사하강법__\n",
    "> BGD. Batch Gradient Descent\n",
    "\n",
    "학습마다 모든 데이터의 미분값을 계산하는 방식입니다.  \n",
    "안정적으로 최솟값을 찾을 수 있지만, 지역최솟값을 경우 이를 벗어나기 힘듭니다.  \n",
    "또한 데이터양이 많아질수록 오래걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.2 확률적 경사하강법__\n",
    "> SGD. Stochastic Gradient Descent\n",
    "\n",
    "한번의 학습에 데이트 세트 중 하나의 샘플을 랜덤하게 골라 그것만의 미분값을 계산하는 방식입니다.  \n",
    "> 랜덤하게 골라야 전체 데이터 세트에 대해 편향을 최소화하고 대표할 수 있습니다.  \n",
    "\n",
    "데이터 양에 상관없이 빠르기 때문에 편리합니다.  \n",
    "배치 방식에 비해 불안정하여 최적의 값은 찾기 힘들지만, 그로 인해 지역최솟값을 피할 확률도 높습니다.  \n",
    "따라서 학습률을 점진적으로 감소시켜 지역최솟값을 피하면서 최종적으로는 어떤 최솟값에 근접하도록 할 수 있습니다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1.2.3 미니배치 경사하강법__\n",
    "데이터 세트 중에 임의의 샘플세트로 미분값을 계산하여 조정하는 방식입니다.  \n",
    "> SGD에 비해, 행렬연산이 강한 GPU를 사용할 시 얻는 성능향상이 큽니다.  \n",
    "\n",
    "SGD보다 안정적으로 조정하여 지역최솟값에 도달하기도 쉽지만, 어떤 최솟값에 더욱 근접하게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.3. 다항회귀__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "각 샘플마다 값의 제곱, 교차항의 곱을 추가하여 선형회귀를 계산하는 방식입니다.  \n",
    "이렇게하면 직선으로 표현 불가능한 <b>비선형 데이터세트</b>를 모델링할 수 있습니다.  \n",
    "> 교차항의 값이 추가되기 때문에 각 특성간의 관계 또한 찾을 수 있습니다.  \n",
    "\n",
    "> `sklearn.preprocessing.PolynomialFeatures'로 데이터를 변환한 뒤 선형회귀를 합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플마다 값의 제곱, 교차항의 곱을 추가하여 선형회귀를 계산하는 방식입니다.  \n",
    "이렇게하면 직선으로 표현 불가능한 <b>비선형 데이터세트</b>를 모델링할 수 있습니다.  \n",
    "> 교차항의 값이 추가되기 때문에 각 특성간의 관계 또한 찾을 수 있습니다.  \n",
    "\n",
    "> `sklearn.preprocessing.PolynomialFeatures'로 데이터를 변환한 뒤 선형회귀를 합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2 적합성과 규제__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2.1 과소적합__\n",
    "모델이 학습을 덜 하여, 훈련/검증 데이터 간의 성능 차이는 적지만, 성능 자체가 낮은 경우입니다.  \n",
    "학습샘플의 양은 중요하지 않으며, 더 복잡한 모델이나 특성을 추가하여 개선할 수 있습니다.\n",
    "> 각 성능을 반복횟수(훈련세트 크기)로 그래프를 그려, 성능이 나쁘고 간격이 적으면 과소적합입니다.  \n",
    "\n",
    "### __2.2 과대적합__\n",
    "모델이 학습데이터만 과도하게 조정되어 훈련데이터의 성능은 좋지만 검증데이터의 성능이 나쁜 경우입니다.  \n",
    "고차 다항회귀 등과 같이 모델이 복잡한 경우에 이러한 경향을 띠며,  \n",
    "모델의 복잡도를 낮추거나, 학습샘플를 더 많이 추가하면 개선할 수 있습니다.\n",
    "> 각 성능을 반복횟수(훈련세트 크기)로 그래프를 그려, 간격이 넓으면 과대적합입니다.  \n",
    "\n",
    "### __2.3 규제__\n",
    "모델의 복잡도를 줄이는 방법들을 총칭합니다.  \n",
    "다항회귀의 경우 차수를 낮추고, 선형회귀의 경우 가중치를 제한합니다.  \n",
    "손실함수와 성능지표가 같아도 규제는 훈련하는 동안만 사용하며, 최종 평가는 규제 없이 평가합니다.  \n",
    "#### __2.3.1 릿지 회귀 (티호노프 규제)__\n",
    "규제항 $\\alpha \\sum_{i=1}^{n}{\\theta_{i}^2}$을 손실함수에 더하는 방식입니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704561b0d644309cd2ab0220a5b729490d45ccecc249cd05cdfebaf802484c8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
