{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. 앙상블__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.ensanble`  \n",
    "<b>앙상블</b>은 여러개의 모델을 통해 최선의 결과를 도출하는 방법입니다.  \n",
    "일반적으로 단일의 모델이 보다 앙상블을 통해 나온 결과가 좋은 경우가 많습니다. \n",
    "> 편향은 비슷하지만, 분산이 줄어드는 경향이 있습니다.  \n",
    "\n",
    "### __1.1 직접/간접 투표__\n",
    "<b>직접투표</b><span style=\"font-size:70%\">hard voting</span>는 여러 모델의 값을 가지고 다수결로 최종 값을 정하는 것입니다.  \n",
    "\n",
    "보통 앙상블에 포함된 가장 우수한 모델보다 성능이 좋으며,  \n",
    "각각의 모델이 성능이 안좋아도 앙상블 모델은 좋은 성능을 보일 수 있습니다.  \n",
    "> 통계학의 <b>대수의 법칙</b>에 따라 모델(시행)이 많을 수록 정답(모수)과 가까워지는 성질 때문입니다.  \n",
    "  모든 시행이 독립적을 때의 얘기이므로 앙상블의 모델이 각기 독립적일수록 좋습니다.  \n",
    "  훈련 데이터를 다르게 하거나, 다른 알고리즘을 사용하면 됩니다.  \n",
    "\n",
    "<b>간접투표</b><span style=\"font-size:70%\">soft voting</span>는 평균으로 값을 정합니다.  \n",
    "회귀에 적합하며, 분류의 경우에도 모델이 확률값을 가지면 직접투표보다 성능이 좋은 경우가 있습니다.  \n",
    "\n",
    "\n",
    "### __1.2 앙상블 방법__\n",
    "<b>배깅</b>은 중복을 허용하여 전체 데이터중 일부를 랜덤추출로 사용하며,  \n",
    "<b>페이스팅</b>은 중복을 하지 않습니다.  \n",
    "> 배깅은 중복데이터로 편향이 늘지만(비독립적), 분산을 더욱 감소시켜 일반적으로 선호됩니다.  \n",
    "\n",
    "<b>부트스트랩</b>은 여러 모델을 병렬적으로 처리하는걸 의미합니다.  \n",
    "\n",
    "\n",
    "## __1.3 특성 앙상블__\n",
    "샘플이 아닌 특성을 일부만 사용하여 모델을 만들기도 합니다.  \n",
    "주로 이미지 등의 특성이 매우 많은 경우에 좋습니다.  \n",
    "> 분산이 줄지만, 특성 앙상블은 편향이 늘어납니다.  \n",
    "\n",
    "<b>랜덤패치 방식</b>은 샘플과 특성 모두 일부만 사용하는 방식이며,  \n",
    "<b>랜덤 서브스페이스 방식</b>은 전체 샘플을 사용하며 특성만 일부분 샘플링합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2. 랜덤 포레스트__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정트리를 여러개 만들어 앙상블 하는 방법입니다.  \n",
    "`skleanr.ensemble.RandomForest-`은 배깅을 사용하여 구현합니다.  \n",
    "또한, 각 노드에서 사용하는 특성이 불순도를 얼마나 낮추는지 확인하여 특성들의 중요도를 측정할 수 있습니다.  \n",
    "> `_.feature_importances_`  \n",
    " 일반 결정트리는 사용하는 몇 특성의 중요도만 나오지만, 랜덤포레스트는 모든 특성의 중요도가 보다 정확하게 측정됩니다.\n",
    "\n",
    "* __엑스트라 트리(익스트림 랜덤 트리)__  \n",
    " 어차피 많은 트리가 만들어지기 때문에, 처음부터 각 트리의 노드에서 최적의 임계값을 찾는게 아닌  \n",
    " 무작위 값으로 분할을 하는 방식입니다.  \n",
    " 트리에서 임계값을 찾는게 오래걸리기 때문에 이 방법이 빠르고, 랜덤 포레스트와의 성능은 비교를 해봐야 합니다.  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3. 부스팅__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약한 학습기 여러개를 순차적으로 사용하여 점점 성능을 보완하는 방법입니다.  \n",
    "\n",
    "### __3.1 AdaBoost,__ (Adaptive-)  \n",
    "이전 모델에서 과소적합했던(잘못된) 샘플에 가중치를 높여 다음 모델을 개선합니다.  \n",
    "개선된 최종 모델을 사용하는 것이 아닌 앙상블을 하지만, 각 모델별 성능이 높을수록 가중치를 높게 주어 최종 값을 계산합니다.  \n",
    "기본적인 에이다부스트는 이진분류에 사용됩니다.  \n",
    "\n",
    "`sklearn`에서는 __SAMME__ 알고리즘을 사용하여 3개 이상의 클래스를 다룹니다.  \n",
    "\n",
    "* __학습 알고리즘__  \n",
    "각 샘플의 초기 가중치 $w_0$은 $1\\over m$으로, 아래의 식대로 계산합니다.  \n",
    "$w_{i+1}=\\begin{cases} w_i & \\hat{y_i}=y_i \\\\ w_{i} exp(\\alpha) & \\hat{y_i} \\neq y_i \\end{cases}$  \n",
    "모델 가중치, $\\alpha = \\eta log {1-r \\over r}$  \n",
    "에러율, $r={\\sum{w_i} (\\hat{y_i} \\neq y_i) \\over \\sum{w_i}}$  \n",
    "실제값과 예측값이 다른 샘플이 많을수록 모델 가중치가 적어집니다.   \n",
    "\n",
    "* __예측 알고리즘__\n",
    "$argmax_{k} \\; \\sum{\\alpha}$\n",
    "\n",
    "### __3.2 Gradient Boost__\n",
    "이전 모델의 잔차를 이용하여 다음 모델을 개선하는 방법입니다.  \n",
    "\n",
    "처음 모델을 만들고 2번째 모델 부터는 진짜 목표값이 아닌 첫번째 모델의 잔차를 목표값으로 하여 학습을 합니다.  \n",
    "이렇게 만들어진 모델들의 합이 최종 모델이 됩니다.  \n",
    "트리가 매우 많아지면 과대적합되기 쉽기에 조기종료 등의 방법을 고려해야합니다.  \n",
    "\n",
    "* __XGBoost__  \n",
    "그래디언트 부스팅의 최적화 알고리즘입니다.  \n",
    "과적합에 대한 규제가 있고, 병렬처리 역시 가능합니다.  \n",
    "Level-wise 방식으로, 넓이 우선 방식으로 트리를 만듭니다.  \n",
    "\n",
    "* __CatBoost__  \n",
    "일부의 훈련데이터의 잔차만을 가지고 그래디언트 부스팅을 하는 방식입니다.  \n",
    "XGBoost와 동일하게 Level-wise 방식입니다.  \n",
    "\n",
    "범주형 데이터가 많을 때 효과적이며, 범주형 변수의 처리는 기본적으로 one-hot이지만,  \n",
    "어떤 범주의 종류(cardinality)가 많으면 Response encoding방식으로 처리합니다.  \n",
    "> Response encoding: 범주형 변수의 처리 방식으로, 전시간(혹은 이전데이터)의 같은 범주의 값들의 평균으로 인코딩합니다.  \n",
    "\n",
    "또한 시계열 데이터에 적합합니다.  \n",
    "Response의 처리방식도 시간 데이터가 있으면 시간순으로 처리하고,  \n",
    "잔차를 이용하는 방식도 시간순으로 처리하기 때문입니다.  \n",
    "> Orderd Boosting: 시간순으로 일부분의 데이터로 모델을 만들고, 이 모델의 잔차로 이후 모델을 만들어 갑니다.  \n",
    "\n",
    "마지막 장점은 모델의 동작 방식 자체가 규제와 비슷한 효과를 주어 파라미터 튜닝을 신경쓰지 않아도 됩니다.  \n",
    "\n",
    "하지만 수치형 데이터가 대부분이면 LightGBM보다 느리며, 희소행렬(sparse) 데이터는 처리가 불가능합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704561b0d644309cd2ab0220a5b729490d45ccecc249cd05cdfebaf802484c8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
